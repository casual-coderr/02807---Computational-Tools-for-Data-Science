{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd8234e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:11:48.211002Z",
     "iopub.status.busy": "2025-11-27T20:11:48.210737Z",
     "iopub.status.idle": "2025-11-27T20:11:49.620147Z",
     "shell.execute_reply": "2025-11-27T20:11:49.619224Z",
     "shell.execute_reply.started": "2025-11-27T20:11:48.210979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: requests in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: mlxtend in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (0.23.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (1.16.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from mlxtend) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rishi\\miniforge3\\envs\\ml\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy requests matplotlib seaborn mlxtend scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a73a786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:11:49.621114Z",
     "iopub.status.busy": "2025-11-27T20:11:49.620908Z",
     "iopub.status.idle": "2025-11-27T20:11:50.633580Z",
     "shell.execute_reply": "2025-11-27T20:11:50.632725Z",
     "shell.execute_reply.started": "2025-11-27T20:11:49.621093Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c14b3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:11:50.634796Z",
     "iopub.status.busy": "2025-11-27T20:11:50.634410Z",
     "iopub.status.idle": "2025-11-27T20:12:58.270233Z",
     "shell.execute_reply": "2025-11-27T20:12:58.269495Z",
     "shell.execute_reply.started": "2025-11-27T20:11:50.634776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trips_2018.csv already exists locally\n",
      "Trips: 17548339  |  Date span: 2018-01-01 → 2018-12-31\n",
      "Trips: 17548339  |  Date span: 2018-01-01 → 2018-12-31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tripduration</th>\n",
       "      <th>starttime</th>\n",
       "      <th>stoptime</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>bikeid</th>\n",
       "      <th>usertype</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>gender</th>\n",
       "      <th>start_hour</th>\n",
       "      <th>trip_duration_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>970</td>\n",
       "      <td>2018-01-01 13:50:57.434</td>\n",
       "      <td>2018-01-01 14:07:08.186</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>505.0</td>\n",
       "      <td>40.749013</td>\n",
       "      <td>-73.988484</td>\n",
       "      <td>31956</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 13:00:00</td>\n",
       "      <td>16.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>723</td>\n",
       "      <td>2018-01-01 15:33:30.182</td>\n",
       "      <td>2018-01-01 15:45:33.341</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>3255.0</td>\n",
       "      <td>40.750585</td>\n",
       "      <td>-73.994685</td>\n",
       "      <td>32536</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1969</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 15:00:00</td>\n",
       "      <td>12.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>496</td>\n",
       "      <td>2018-01-01 15:39:18.337</td>\n",
       "      <td>2018-01-01 15:47:35.172</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>525.0</td>\n",
       "      <td>40.755942</td>\n",
       "      <td>-74.002116</td>\n",
       "      <td>16069</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1956</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 15:00:00</td>\n",
       "      <td>8.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>306</td>\n",
       "      <td>2018-01-01 15:40:13.372</td>\n",
       "      <td>2018-01-01 15:45:20.191</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>447.0</td>\n",
       "      <td>40.763707</td>\n",
       "      <td>-73.985162</td>\n",
       "      <td>31781</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1974</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 15:00:00</td>\n",
       "      <td>5.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306</td>\n",
       "      <td>2018-01-01 18:14:51.568</td>\n",
       "      <td>2018-01-01 18:19:57.642</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>3356.0</td>\n",
       "      <td>40.774667</td>\n",
       "      <td>-73.984706</td>\n",
       "      <td>30319</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 18:00:00</td>\n",
       "      <td>5.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tripduration               starttime                stoptime  \\\n",
       "0           970 2018-01-01 13:50:57.434 2018-01-01 14:07:08.186   \n",
       "1           723 2018-01-01 15:33:30.182 2018-01-01 15:45:33.341   \n",
       "2           496 2018-01-01 15:39:18.337 2018-01-01 15:47:35.172   \n",
       "3           306 2018-01-01 15:40:13.372 2018-01-01 15:45:20.191   \n",
       "4           306 2018-01-01 18:14:51.568 2018-01-01 18:19:57.642   \n",
       "\n",
       "   start_station_id  start_station_latitude  start_station_longitude  \\\n",
       "0              72.0               40.767272               -73.993929   \n",
       "1              72.0               40.767272               -73.993929   \n",
       "2              72.0               40.767272               -73.993929   \n",
       "3              72.0               40.767272               -73.993929   \n",
       "4              72.0               40.767272               -73.993929   \n",
       "\n",
       "   end_station_id  end_station_latitude  end_station_longitude  bikeid  \\\n",
       "0           505.0             40.749013             -73.988484   31956   \n",
       "1          3255.0             40.750585             -73.994685   32536   \n",
       "2           525.0             40.755942             -74.002116   16069   \n",
       "3           447.0             40.763707             -73.985162   31781   \n",
       "4          3356.0             40.774667             -73.984706   30319   \n",
       "\n",
       "     usertype  birth_year  gender          start_hour  trip_duration_min  \n",
       "0  Subscriber        1992       1 2018-01-01 13:00:00          16.166667  \n",
       "1  Subscriber        1969       1 2018-01-01 15:00:00          12.050000  \n",
       "2  Subscriber        1956       1 2018-01-01 15:00:00           8.266667  \n",
       "3  Subscriber        1974       1 2018-01-01 15:00:00           5.100000  \n",
       "4  Subscriber        1992       1 2018-01-01 18:00:00           5.100000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Configuration: URL to download CSV from\n",
    "CSV_URL = \"https://pub-c5da5968baf1476f998733ecc980092f.r2.dev/Trips_2018.csv\"\n",
    "CSV_FILE = \"Trips_2018.csv\"\n",
    "\n",
    "# Download CSV if not present locally\n",
    "if not os.path.exists(CSV_FILE):\n",
    "    print(f\"Downloading {CSV_FILE} from {CSV_URL}...\")\n",
    "    import requests\n",
    "    \n",
    "    response = requests.get(CSV_URL, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    print(f\"File size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    with open(CSV_FILE, 'wb') as f:\n",
    "        downloaded = 0\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "                downloaded += len(chunk)\n",
    "                if total_size:\n",
    "                    progress = (downloaded / total_size) * 100\n",
    "                    print(f\"Progress: {progress:.1f}%\", end='\\r')\n",
    "    \n",
    "    print(f\"\\n✓ Download complete!\")\n",
    "else:\n",
    "    print(f\"✓ {CSV_FILE} already exists locally\")\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "#Remove unwanted index column if it exists\n",
    "if \"Unnamed: 0\" in df.columns:\n",
    "    df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "#Convert start and stop times to proper datetime format\n",
    "df[\"starttime\"] = pd.to_datetime(df[\"starttime\"], errors=\"coerce\")\n",
    "df[\"stoptime\"]  = pd.to_datetime(df.get(\"stoptime\"), errors=\"coerce\")\n",
    "\n",
    "\n",
    "#Create a new column 'start_hour' — the trip's start time rounded down to the hour (used later to join with hourly weather data)\n",
    "df[\"start_hour\"] = df[\"starttime\"].dt.floor(\"h\")\n",
    "\n",
    "#Compute trip duration in minutes\n",
    "df[\"trip_duration_min\"] = (\n",
    "    df[\"tripduration\"]/60.0\n",
    "    if \"tripduration\" in df.columns\n",
    "    else (df[\"stoptime\"] - df[\"starttime\"]).dt.total_seconds()/60\n",
    ")\n",
    "\n",
    "#Find a representative latitude and longitude for weather data Using median coordinates ensures one central location (e.g., NYC center)\n",
    "LAT, LON = df[\"start_station_latitude\"].median(), df[\"start_station_longitude\"].median()\n",
    "\n",
    "#Find the overall date range of the dataset for API query\n",
    "START = df[\"start_hour\"].min().date().isoformat()\n",
    "END   = df[\"start_hour\"].max().date().isoformat()\n",
    "\n",
    "#Print a quick summary\n",
    "print(f\"Trips: {len(df)}  |  Date span: {START} → {END}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b263370f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:12:58.271321Z",
     "iopub.status.busy": "2025-11-27T20:12:58.271124Z",
     "iopub.status.idle": "2025-11-27T20:13:15.943855Z",
     "shell.execute_reply": "2025-11-27T20:13:15.942997Z",
     "shell.execute_reply.started": "2025-11-27T20:12:58.271303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique grid cells to fetch: 26\n",
      "Weather rows (cells): 227760 | cells covered: 26\n",
      "Weather rows (cells): 227760 | cells covered: 26\n",
      "Total weather rows (expanded to stations): 8050440\n",
      "Total weather rows (expanded to stations): 8050440\n"
     ]
    }
   ],
   "source": [
    "# --- Weather by small spatial grid (fast, robust, with fallback) ---\n",
    "stations = df[[\"start_station_id\", \"start_station_latitude\", \"start_station_longitude\"]].drop_duplicates()\n",
    "\n",
    "# 1) Build a small grid (≈2–3 km) to reduce API calls\n",
    "GRID = 0.03  # ↑ 0.05 fewer calls, ↓ 0.02 more detail\n",
    "\n",
    "stations = stations.copy()\n",
    "stations[\"lat_bin\"] = (stations[\"start_station_latitude\"]  / GRID).round().astype(int)\n",
    "stations[\"lon_bin\"] = (stations[\"start_station_longitude\"] / GRID).round().astype(int)\n",
    "stations[\"cell_id\"] = stations[\"lat_bin\"].astype(str) + \"_\" + stations[\"lon_bin\"].astype(str)\n",
    "\n",
    "cells = (stations.groupby(\"cell_id\", as_index=False)\n",
    "         .agg(lat=(\"start_station_latitude\", \"median\"),\n",
    "              lon=(\"start_station_longitude\", \"median\")))\n",
    "\n",
    "print(f\"Unique grid cells to fetch: {len(cells)}\")\n",
    "\n",
    "\n",
    "# 2) Helper: polite, retrying fetch per cell\n",
    "def fetch_cell(lat, lon, start_date, end_date, retries=5, pause=1.5):\n",
    "    url = \"https://archive-api.open-meteo.com/v1/era5\"\n",
    "    hourly_vars = [\n",
    "        \"temperature_2m\",\"apparent_temperature\",\"rain\",\"snowfall\",\n",
    "        \"wind_speed_10m\",\"relative_humidity_2m\",\"cloud_cover\",\"visibility\"\n",
    "    ]\n",
    "    params = {\n",
    "        \"latitude\": float(lat), \"longitude\": float(lon),\n",
    "        \"start_date\": start_date, \"end_date\": end_date,\n",
    "        \"timezone\": \"America/New_York\",\n",
    "        \"hourly\": \",\".join(hourly_vars)  # more robust than list for some clients\n",
    "    }\n",
    "    last_err = None\n",
    "    for a in range(retries):\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, timeout=40)\n",
    "            if resp.ok:\n",
    "                js = resp.json()\n",
    "                if \"hourly\" in js:\n",
    "                    wx = pd.DataFrame(js[\"hourly\"])\n",
    "                    wx[\"start_hour\"] = pd.to_datetime(wx[\"time\"])\n",
    "                    wx = wx.drop(columns=[\"time\"])\n",
    "                    # downcast floats to save memory\n",
    "                    for c in wx.columns:\n",
    "                        if c != \"start_hour\" and pd.api.types.is_float_dtype(wx[c]):\n",
    "                            wx[c] = pd.to_numeric(wx[c], downcast=\"float\")\n",
    "                    return wx\n",
    "                else:\n",
    "                    last_err = f\"no 'hourly' in response (keys={list(js.keys())})\"\n",
    "            else:\n",
    "                last_err = f\"HTTP {resp.status_code}: {resp.text[:160]}\"\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "        time.sleep(pause * (a + 1))  # gentle backoff\n",
    "    # Uncomment for debugging:\n",
    "    # print(f\"Fetch failed for ({lat:.4f},{lon:.4f}): {last_err}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# 3) Fetch once per cell\n",
    "cell_weather, failed_cells = [], []\n",
    "for _, r in cells.iterrows():\n",
    "    wx_cell = fetch_cell(r[\"lat\"], r[\"lon\"], START, END)\n",
    "    if wx_cell is None:\n",
    "        failed_cells.append(r[\"cell_id\"])\n",
    "        continue\n",
    "    wx_cell[\"cell_id\"] = r[\"cell_id\"]\n",
    "    cell_weather.append(wx_cell)\n",
    "    time.sleep(0.2)  # pacing\n",
    "\n",
    "# --- Fallback if everything failed: use single (city-median) point ---\n",
    "if not cell_weather:\n",
    "    print(\"⚠️ All grid cells failed. Falling back to city-median weather…\")\n",
    "    lat0 = float(df[\"start_station_latitude\"].median())\n",
    "    lon0 = float(df[\"start_station_longitude\"].median())\n",
    "    wx_city = fetch_cell(lat0, lon0, START, END)\n",
    "    if wx_city is None:\n",
    "        raise RuntimeError(\n",
    "            f\"No weather fetched even for city median. Check network/date range. START={START}, END={END}\"\n",
    "        )\n",
    "    wx_city[\"cell_id\"] = \"city_median\"\n",
    "    wx_cells = wx_city.copy()\n",
    "    stations2 = stations.copy()\n",
    "    stations2[\"use_cell\"] = \"city_median\"\n",
    "else:\n",
    "    wx_cells = pd.concat(cell_weather, ignore_index=True)\n",
    "    print(f\"Weather rows (cells): {len(wx_cells)} | cells covered: {wx_cells['cell_id'].nunique()}\")\n",
    "\n",
    "    # 4) If some cells failed, borrow weather from nearest fetched cell\n",
    "    if failed_cells:\n",
    "        print(\"Filling missing cells from nearest fetched cell…\")\n",
    "\n",
    "        have = cells[cells[\"cell_id\"].isin(wx_cells[\"cell_id\"].unique())].copy()\n",
    "        need = cells[cells[\"cell_id\"].isin(failed_cells)].copy()\n",
    "\n",
    "        def haversine(lat1, lon1, lat2, lon2):\n",
    "            R = 6371.0\n",
    "            lat1, lon1, lat2, lon2 = map(np.radians, (lat1, lon1, lat2, lon2))\n",
    "            dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "            a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "            return 2 * R * np.arcsin(np.sqrt(a))\n",
    "\n",
    "        alias_rows = []\n",
    "        H_lat = have[\"lat\"].to_numpy()\n",
    "        H_lon = have[\"lon\"].to_numpy()\n",
    "        for _, r in need.iterrows():\n",
    "            d = haversine(r[\"lat\"], r[\"lon\"], H_lat, H_lon)\n",
    "            nearest = have.iloc[d.argmin()][\"cell_id\"]\n",
    "            alias_rows.append((r[\"cell_id\"], nearest))\n",
    "        alias = pd.DataFrame(alias_rows, columns=[\"cell_id\", \"use_cell\"])\n",
    "    else:\n",
    "        alias = pd.DataFrame(columns=[\"cell_id\", \"use_cell\"])\n",
    "\n",
    "    # Stations use their own cell if fetched, otherwise the nearest fetched cell\n",
    "    stations2 = stations.merge(alias, on=\"cell_id\", how=\"left\")\n",
    "    stations2[\"use_cell\"] = stations2[\"use_cell\"].fillna(stations2[\"cell_id\"])\n",
    "\n",
    "# 5) Expand cell weather back to stations\n",
    "wx = (stations2[[\"start_station_id\", \"use_cell\"]]\n",
    "      .rename(columns={\"use_cell\": \"cell_id\"})\n",
    "      .merge(wx_cells, on=\"cell_id\", how=\"left\"))\n",
    "\n",
    "print(\"Total weather rows (expanded to stations):\", len(wx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72066897",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:13:15.944623Z",
     "iopub.status.busy": "2025-11-27T20:13:15.944427Z",
     "iopub.status.idle": "2025-11-27T20:13:26.598640Z",
     "shell.execute_reply": "2025-11-27T20:13:26.597825Z",
     "shell.execute_reply.started": "2025-11-27T20:13:15.944605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging on: ['start_hour', 'start_station_id']\n",
      "Merge complete | rows=17,548,339 | weather coverage=100.0%\n",
      "Merge complete | rows=17,548,339 | weather coverage=100.0%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tripduration</th>\n",
       "      <th>starttime</th>\n",
       "      <th>stoptime</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>bikeid</th>\n",
       "      <th>...</th>\n",
       "      <th>start_hour</th>\n",
       "      <th>trip_duration_min</th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>apparent_temperature</th>\n",
       "      <th>rain</th>\n",
       "      <th>snowfall</th>\n",
       "      <th>wind_speed_10m</th>\n",
       "      <th>relative_humidity_2m</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>visibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>970</td>\n",
       "      <td>2018-01-01 13:50:57.434</td>\n",
       "      <td>2018-01-01 14:07:08.186</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>505.0</td>\n",
       "      <td>40.749013</td>\n",
       "      <td>-73.988484</td>\n",
       "      <td>31956</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-01-01 13:00:00</td>\n",
       "      <td>16.166667</td>\n",
       "      <td>-8.1</td>\n",
       "      <td>-14.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>723</td>\n",
       "      <td>2018-01-01 15:33:30.182</td>\n",
       "      <td>2018-01-01 15:45:33.341</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>3255.0</td>\n",
       "      <td>40.750585</td>\n",
       "      <td>-73.994685</td>\n",
       "      <td>32536</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-01-01 15:00:00</td>\n",
       "      <td>12.050000</td>\n",
       "      <td>-6.9</td>\n",
       "      <td>-13.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>496</td>\n",
       "      <td>2018-01-01 15:39:18.337</td>\n",
       "      <td>2018-01-01 15:47:35.172</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>525.0</td>\n",
       "      <td>40.755942</td>\n",
       "      <td>-74.002116</td>\n",
       "      <td>16069</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-01-01 15:00:00</td>\n",
       "      <td>8.266667</td>\n",
       "      <td>-6.9</td>\n",
       "      <td>-13.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>306</td>\n",
       "      <td>2018-01-01 15:40:13.372</td>\n",
       "      <td>2018-01-01 15:45:20.191</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>447.0</td>\n",
       "      <td>40.763707</td>\n",
       "      <td>-73.985162</td>\n",
       "      <td>31781</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-01-01 15:00:00</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>-6.9</td>\n",
       "      <td>-13.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.700001</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306</td>\n",
       "      <td>2018-01-01 18:14:51.568</td>\n",
       "      <td>2018-01-01 18:19:57.642</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>3356.0</td>\n",
       "      <td>40.774667</td>\n",
       "      <td>-73.984706</td>\n",
       "      <td>30319</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-01-01 18:00:00</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>39</td>\n",
       "      <td>23</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tripduration               starttime                stoptime  \\\n",
       "0           970 2018-01-01 13:50:57.434 2018-01-01 14:07:08.186   \n",
       "1           723 2018-01-01 15:33:30.182 2018-01-01 15:45:33.341   \n",
       "2           496 2018-01-01 15:39:18.337 2018-01-01 15:47:35.172   \n",
       "3           306 2018-01-01 15:40:13.372 2018-01-01 15:45:20.191   \n",
       "4           306 2018-01-01 18:14:51.568 2018-01-01 18:19:57.642   \n",
       "\n",
       "   start_station_id  start_station_latitude  start_station_longitude  \\\n",
       "0              72.0               40.767272               -73.993929   \n",
       "1              72.0               40.767272               -73.993929   \n",
       "2              72.0               40.767272               -73.993929   \n",
       "3              72.0               40.767272               -73.993929   \n",
       "4              72.0               40.767272               -73.993929   \n",
       "\n",
       "   end_station_id  end_station_latitude  end_station_longitude  bikeid  ...  \\\n",
       "0           505.0             40.749013             -73.988484   31956  ...   \n",
       "1          3255.0             40.750585             -73.994685   32536  ...   \n",
       "2           525.0             40.755942             -74.002116   16069  ...   \n",
       "3           447.0             40.763707             -73.985162   31781  ...   \n",
       "4          3356.0             40.774667             -73.984706   30319  ...   \n",
       "\n",
       "           start_hour  trip_duration_min  temperature_2m apparent_temperature  \\\n",
       "0 2018-01-01 13:00:00          16.166667            -8.1                -14.9   \n",
       "1 2018-01-01 15:00:00          12.050000            -6.9                -13.7   \n",
       "2 2018-01-01 15:00:00           8.266667            -6.9                -13.7   \n",
       "3 2018-01-01 15:00:00           5.100000            -6.9                -13.7   \n",
       "4 2018-01-01 18:00:00           5.100000           -10.0                -15.0   \n",
       "\n",
       "   rain  snowfall  wind_speed_10m  relative_humidity_2m  cloud_cover  \\\n",
       "0   0.0       0.0       20.400000                    30            0   \n",
       "1   0.0       0.0       19.700001                    24           55   \n",
       "2   0.0       0.0       19.700001                    24           55   \n",
       "3   0.0       0.0       19.700001                    24           55   \n",
       "4   0.0       0.0        7.400000                    39           23   \n",
       "\n",
       "   visibility  \n",
       "0        None  \n",
       "1        None  \n",
       "2        None  \n",
       "3        None  \n",
       "4        None  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Merge trips with hourly weather ===\n",
    "\n",
    "is_station_level = \"start_station_id\" in wx.columns\n",
    "key_cols = [\"start_hour\"] + ([\"start_station_id\"] if is_station_level else [])\n",
    "\n",
    "print(\"Merging on:\", key_cols)\n",
    "\n",
    "# keep only weather fields we actually need (saves memory) if present\n",
    "wanted_wx = [\n",
    "    \"start_hour\", \"start_station_id\",\n",
    "    \"temperature_2m\",\"apparent_temperature\",\"rain\",\"snowfall\",\n",
    "    \"wind_speed_10m\",\"relative_humidity_2m\",\"cloud_cover\",\"visibility\"\n",
    "]\n",
    "wx_keyed = wx[[c for c in wanted_wx if c in wx.columns]].drop_duplicates(subset=key_cols)\n",
    "\n",
    "dfm = df.merge(\n",
    "    wx_keyed,\n",
    "    on=key_cols,\n",
    "    how=\"left\",          # keep all trips/columns from df\n",
    "    validate=\"m:1\"       # many trips to one weather row\n",
    ")\n",
    "\n",
    "# merge quality\n",
    "miss_pct = dfm[\"temperature_2m\"].isna().mean() * 100\n",
    "print(f\"Merge complete | rows={len(dfm):,} | weather coverage={100 - miss_pct:.1f}%\")\n",
    "\n",
    "dfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1552c915",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:13:26.599405Z",
     "iopub.status.busy": "2025-11-27T20:13:26.599214Z",
     "iopub.status.idle": "2025-11-27T20:13:29.970414Z",
     "shell.execute_reply": "2025-11-27T20:13:29.969486Z",
     "shell.execute_reply.started": "2025-11-27T20:13:26.599387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather features added | shape=(17548339, 30) | new cols: ['temp_celsius','temp_category','is_dry','wind_kmh','sky_condition','visibility_km','cycling_score']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tripduration</th>\n",
       "      <th>starttime</th>\n",
       "      <th>stoptime</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>bikeid</th>\n",
       "      <th>...</th>\n",
       "      <th>relative_humidity_2m</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>visibility</th>\n",
       "      <th>temp_celsius</th>\n",
       "      <th>temp_category</th>\n",
       "      <th>is_dry</th>\n",
       "      <th>wind_kmh</th>\n",
       "      <th>sky_condition</th>\n",
       "      <th>visibility_km</th>\n",
       "      <th>cycling_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>970</td>\n",
       "      <td>2018-01-01 13:50:57.434</td>\n",
       "      <td>2018-01-01 14:07:08.186</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>505.0</td>\n",
       "      <td>40.749013</td>\n",
       "      <td>-73.988484</td>\n",
       "      <td>31956</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8.1</td>\n",
       "      <td>freezing</td>\n",
       "      <td>1</td>\n",
       "      <td>73.439995</td>\n",
       "      <td>clear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.025681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>723</td>\n",
       "      <td>2018-01-01 15:33:30.182</td>\n",
       "      <td>2018-01-01 15:45:33.341</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>3255.0</td>\n",
       "      <td>40.750585</td>\n",
       "      <td>-73.994685</td>\n",
       "      <td>32536</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-6.9</td>\n",
       "      <td>freezing</td>\n",
       "      <td>1</td>\n",
       "      <td>70.919998</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.049688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>496</td>\n",
       "      <td>2018-01-01 15:39:18.337</td>\n",
       "      <td>2018-01-01 15:47:35.172</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>525.0</td>\n",
       "      <td>40.755942</td>\n",
       "      <td>-74.002116</td>\n",
       "      <td>16069</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-6.9</td>\n",
       "      <td>freezing</td>\n",
       "      <td>1</td>\n",
       "      <td>70.919998</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.049688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>306</td>\n",
       "      <td>2018-01-01 15:40:13.372</td>\n",
       "      <td>2018-01-01 15:45:20.191</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>447.0</td>\n",
       "      <td>40.763707</td>\n",
       "      <td>-73.985162</td>\n",
       "      <td>31781</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-6.9</td>\n",
       "      <td>freezing</td>\n",
       "      <td>1</td>\n",
       "      <td>70.919998</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.049688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306</td>\n",
       "      <td>2018-01-01 18:14:51.568</td>\n",
       "      <td>2018-01-01 18:19:57.642</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>3356.0</td>\n",
       "      <td>40.774667</td>\n",
       "      <td>-73.984706</td>\n",
       "      <td>30319</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>freezing</td>\n",
       "      <td>1</td>\n",
       "      <td>26.639999</td>\n",
       "      <td>clear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.008514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tripduration               starttime                stoptime  \\\n",
       "0           970 2018-01-01 13:50:57.434 2018-01-01 14:07:08.186   \n",
       "1           723 2018-01-01 15:33:30.182 2018-01-01 15:45:33.341   \n",
       "2           496 2018-01-01 15:39:18.337 2018-01-01 15:47:35.172   \n",
       "3           306 2018-01-01 15:40:13.372 2018-01-01 15:45:20.191   \n",
       "4           306 2018-01-01 18:14:51.568 2018-01-01 18:19:57.642   \n",
       "\n",
       "   start_station_id  start_station_latitude  start_station_longitude  \\\n",
       "0              72.0               40.767272               -73.993929   \n",
       "1              72.0               40.767272               -73.993929   \n",
       "2              72.0               40.767272               -73.993929   \n",
       "3              72.0               40.767272               -73.993929   \n",
       "4              72.0               40.767272               -73.993929   \n",
       "\n",
       "   end_station_id  end_station_latitude  end_station_longitude  bikeid  ...  \\\n",
       "0           505.0             40.749013             -73.988484   31956  ...   \n",
       "1          3255.0             40.750585             -73.994685   32536  ...   \n",
       "2           525.0             40.755942             -74.002116   16069  ...   \n",
       "3           447.0             40.763707             -73.985162   31781  ...   \n",
       "4          3356.0             40.774667             -73.984706   30319  ...   \n",
       "\n",
       "  relative_humidity_2m  cloud_cover  visibility temp_celsius  temp_category  \\\n",
       "0                   30            0         NaN         -8.1       freezing   \n",
       "1                   24           55         NaN         -6.9       freezing   \n",
       "2                   24           55         NaN         -6.9       freezing   \n",
       "3                   24           55         NaN         -6.9       freezing   \n",
       "4                   39           23         NaN        -10.0       freezing   \n",
       "\n",
       "   is_dry   wind_kmh  sky_condition  visibility_km  cycling_score  \n",
       "0       1  73.439995          clear            NaN      20.025681  \n",
       "1       1  70.919998         cloudy            NaN      20.049688  \n",
       "2       1  70.919998         cloudy            NaN      20.049688  \n",
       "3       1  70.919998         cloudy            NaN      20.049688  \n",
       "4       1  26.639999          clear            NaN      20.008514  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Feature engineering: concise, robust, memory-friendly ===\n",
    "\n",
    "need = [\"temperature_2m\",\"rain\",\"snowfall\",\"wind_speed_10m\",\"cloud_cover\",\"visibility\"]\n",
    "for c in need:\n",
    "    if c not in dfm.columns:\n",
    "        dfm[c] = np.nan\n",
    "dfm[need] = dfm[need].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# 1) Temperature (°C) + readable buckets\n",
    "dfm[\"temp_celsius\"] = dfm[\"temperature_2m\"]\n",
    "dfm[\"temp_category\"] = pd.cut(\n",
    "    dfm[\"temp_celsius\"],\n",
    "    bins=[-99, 0, 10, 20, 25, 30, 99],\n",
    "    labels=[\"freezing\",\"cold\",\"cool\",\"comfortable\",\"warm\",\"hot\"]\n",
    ")\n",
    "\n",
    "# 2) Dryness (1 dry, 0 wet)\n",
    "rain  = dfm[\"rain\"].fillna(0)\n",
    "snow  = dfm[\"snowfall\"].fillna(0)\n",
    "dfm[\"is_dry\"] = ((rain + snow) == 0).astype(\"int8\")\n",
    "\n",
    "# 3) Wind, sky, visibility\n",
    "dfm[\"wind_kmh\"] = dfm[\"wind_speed_10m\"] * 3.6\n",
    "dfm[\"sky_condition\"] = pd.cut(\n",
    "    dfm[\"cloud_cover\"],\n",
    "    bins=[-1, 25, 50, 75, 100.1],\n",
    "    labels=[\"clear\",\"partly_cloudy\",\"cloudy\",\"overcast\"]\n",
    ")\n",
    "dfm[\"visibility_km\"] = dfm[\"visibility\"] / 1000.0\n",
    "\n",
    "# 4) Cycling Score (0–100): Weights proportional to empirical correlation coefficients\n",
    "# temp r=0.443, is_dry r=0.128 → normalized to percentages: 69/20/7/4\n",
    "dfm[\"cycling_score\"] = (\n",
    "    np.exp(-((dfm[\"temp_celsius\"] - 20) / 10) ** 2) * 69           # temperature comfort peak ~20°C (69%)\n",
    "    + dfm[\"is_dry\"] * 20                                           # dry bonus (20%)\n",
    "    + (dfm[\"wind_kmh\"] < 20).astype(\"int8\") * 7                    # calm wind (7%)\n",
    "    + (dfm[\"visibility_km\"] > 5).astype(\"int8\") * 4                # clear view (4%)\n",
    ").clip(0, 100)\n",
    "\n",
    "# 5) Compact memory for categories\n",
    "dfm[\"temp_category\"] = dfm[\"temp_category\"].astype(\"category\")\n",
    "dfm[\"sky_condition\"]  = dfm[\"sky_condition\"].astype(\"category\")\n",
    "\n",
    "print(f\"Weather features added | shape={dfm.shape} | new cols: \"\n",
    "      f\"['temp_celsius','temp_category','is_dry','wind_kmh','sky_condition','visibility_km','cycling_score']\")\n",
    "dfm.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19cf09b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:13:29.971099Z",
     "iopub.status.busy": "2025-11-27T20:13:29.970922Z",
     "iopub.status.idle": "2025-11-27T20:13:31.388746Z",
     "shell.execute_reply": "2025-11-27T20:13:31.387834Z",
     "shell.execute_reply.started": "2025-11-27T20:13:29.971082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PREPARING FULL DATASET FOR ANALYSIS\n",
      "======================================================================\n",
      "Dataset ready: 17,548,339 trips with weather and temporal features\n",
      "Dataset ready: 17,548,339 trips with weather and temporal features\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPARING FULL DATASET FOR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Add temporal columns needed for analysis\n",
    "dfm['day_of_week'] = dfm['starttime'].dt.dayofweek\n",
    "dfm['is_weekend'] = (dfm['day_of_week'] >= 5).astype(int)\n",
    "dfm['hour'] = dfm['start_hour'].dt.hour\n",
    "dfm['month'] = dfm['starttime'].dt.month\n",
    "\n",
    "print(f\"Dataset ready: {len(dfm):,} trips with weather and temporal features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d8406c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:13:31.389525Z",
     "iopub.status.busy": "2025-11-27T20:13:31.389340Z",
     "iopub.status.idle": "2025-11-27T20:14:35.883923Z",
     "shell.execute_reply": "2025-11-27T20:14:35.883033Z",
     "shell.execute_reply.started": "2025-11-27T20:13:31.389508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PROCESSING FULL DATASET WITH PARALLEL PROCESSING\n",
      "======================================================================\n",
      "Using 15 CPU cores for parallel processing\n",
      "Processing 17,548,339 trips in batches of 500,000\n",
      "Created 36 batches for parallel processing\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROCESSING FULL DATASET WITH PARALLEL PROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "# Determine optimal number of workers\n",
    "NUM_WORKERS = max(1, cpu_count() - 1)  # Leave one core free\n",
    "BATCH_SIZE = 500000  # Smaller batches for better parallelization\n",
    "\n",
    "print(f\"Using {NUM_WORKERS} CPU cores for parallel processing\")\n",
    "print(f\"Processing {len(dfm):,} trips in batches of {BATCH_SIZE:,}\")\n",
    "\n",
    "# Function to process a single batch\n",
    "def process_batch(batch_data):\n",
    "    \"\"\"Process a batch of trips and return transactions, routes, and profiles\"\"\"\n",
    "    batch, batch_num = batch_data\n",
    "    \n",
    "    # APRIORI: Create transactions (vectorized approach)\n",
    "    batch_clean = batch.dropna(subset=['start_station_id', 'end_station_id']).copy()\n",
    "    \n",
    "    # Vectorized transaction creation\n",
    "    batch_transactions = []\n",
    "    route_counts = {}\n",
    "    \n",
    "    if len(batch_clean) > 0:\n",
    "        # Convert to numpy for faster processing\n",
    "        starts = batch_clean['start_station_id'].astype(int).values\n",
    "        ends = batch_clean['end_station_id'].astype(int).values\n",
    "        hours = batch_clean['start_hour'].dt.hour.values\n",
    "        scores = batch_clean['cycling_score'].values\n",
    "        \n",
    "        for i in range(len(starts)):\n",
    "            start, end = starts[i], ends[i]\n",
    "            \n",
    "            transaction = [\n",
    "                f\"route_{start}_to_{end}\",\n",
    "                f\"start_{start}\",\n",
    "                f\"end_{end}\",\n",
    "            ]\n",
    "            \n",
    "            # Time period\n",
    "            hour = hours[i]\n",
    "            if 7 <= hour <= 9:\n",
    "                transaction.append(\"morning_commute\")\n",
    "            elif 17 <= hour <= 19:\n",
    "                transaction.append(\"evening_commute\")\n",
    "            else:\n",
    "                transaction.append(\"non_commute\")\n",
    "            \n",
    "            # Weather condition\n",
    "            if scores[i] >= 70:\n",
    "                transaction.append(\"good_weather\")\n",
    "            else:\n",
    "                transaction.append(\"poor_weather\")\n",
    "            \n",
    "            batch_transactions.append(transaction)\n",
    "            \n",
    "            # Count routes\n",
    "            route_key = f\"{start}_to_{end}\"\n",
    "            route_counts[route_key] = route_counts.get(route_key, 0) + 1\n",
    "    \n",
    "    # CLUSTERING: Create user profiles (already optimized with groupby)\n",
    "    batch_profiles = batch.groupby(['birth_year', 'gender'], observed=True).agg({\n",
    "        'tripduration': ['mean', 'std', 'count'],\n",
    "        'trip_duration_min': ['mean', 'median'],\n",
    "        'start_station_id': 'nunique',\n",
    "        'end_station_id': 'nunique',\n",
    "        'hour': ['mean', 'std'],\n",
    "        'is_weekend': 'mean',\n",
    "        'day_of_week': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.mean(),\n",
    "        'cycling_score': ['mean', 'std'],\n",
    "        'temp_celsius': 'mean',\n",
    "        'is_dry': 'mean',\n",
    "        'usertype': lambda x: 1 if len(x) > 0 and x.mode()[0] == 'Subscriber' else 0\n",
    "    }).reset_index()\n",
    "    \n",
    "    return batch_transactions, route_counts, batch_profiles\n",
    "\n",
    "# Split data into batches\n",
    "batch_indices = list(range(0, len(dfm), BATCH_SIZE))\n",
    "batches = [(dfm.iloc[i:min(i + BATCH_SIZE, len(dfm))], idx // BATCH_SIZE + 1) \n",
    "           for idx, i in enumerate(batch_indices)]\n",
    "\n",
    "print(f\"Created {len(batches)} batches for parallel processing\")\n",
    "\n",
    "# Process batches in parallel\n",
    "if NUM_WORKERS > 1:\n",
    "    with Pool(processes=NUM_WORKERS) as pool:\n",
    "        results = []\n",
    "        for i, result in enumerate(pool.imap(process_batch, batches), 1):\n",
    "            results.append(result)\n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Completed {i}/{len(batches)} batches\")\n",
    "else:\n",
    "    # Fallback to sequential processing if only 1 core\n",
    "    results = [process_batch(batch) for batch in batches]\n",
    "\n",
    "print(\"Merging results from all batches...\")\n",
    "\n",
    "# Merge results from all batches\n",
    "all_transactions = []\n",
    "route_counter = {}\n",
    "user_profiles_list = []\n",
    "\n",
    "for batch_trans, batch_routes, batch_profiles in results:\n",
    "    all_transactions.extend(batch_trans)\n",
    "    \n",
    "    # Merge route counts\n",
    "    for route, count in batch_routes.items():\n",
    "        route_counter[route] = route_counter.get(route, 0) + count\n",
    "    \n",
    "    user_profiles_list.append(batch_profiles)\n",
    "\n",
    "print(f\"\\nProcessing complete:\")\n",
    "print(f\"  Transactions: {len(all_transactions):,}\")\n",
    "print(f\"  Unique routes: {len(route_counter):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f723636",
   "metadata": {},
   "source": [
    "#  Apriori Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306edace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:14:35.885230Z",
     "iopub.status.busy": "2025-11-27T20:14:35.885014Z",
     "iopub.status.idle": "2025-11-27T20:17:44.687159Z",
     "shell.execute_reply": "2025-11-27T20:17:44.686420Z",
     "shell.execute_reply.started": "2025-11-27T20:14:35.885211Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FULL PIPELINE: PATTERN MINING + VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# A) CHECK IF WE ALREADY MINED RESULTS\n",
    "# ------------------------------------------------------------\n",
    "needed_vars = [\"simplified_trans\", \"frequent_itemsets\", \"rules\",\n",
    "               \"commute_patterns\", \"weather_patterns\",\n",
    "               \"route_counter\", \"all_transactions\"]\n",
    "\n",
    "missing = [v for v in needed_vars if v not in globals()]\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nMining results missing: {missing}\")\n",
    "    print(\"→ Running mining step now...\\n\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 0) Top routes from FULL dataset (descriptive only)\n",
    "    # -------------------------------------------------------\n",
    "    top_10_routes = sorted(route_counter.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    print(\"\\nTop 10 Most Frequent Routes (Full Dataset):\")\n",
    "    total_trips = max(len(all_transactions), 1)\n",
    "    for i, (route, count) in enumerate(top_10_routes, 1):\n",
    "        s1, s2 = route.split(\"_to_\")\n",
    "        pct = count / total_trips * 100\n",
    "        print(f\"  {i:2}. {s1} → {s2}: {count:,} trips ({pct:.2f}%)\")\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 1) Build GEO zones once (quantile grid on lat/lon)\n",
    "    # -------------------------------------------------------\n",
    "    stations_geo = (\n",
    "        dfm[[\"start_station_id\", \"start_station_latitude\", \"start_station_longitude\"]]\n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    stations_geo[\"lat_q\"] = pd.qcut(\n",
    "        stations_geo[\"start_station_latitude\"], q=4, labels=[\"A\", \"B\", \"C\", \"D\"]\n",
    "    )\n",
    "    stations_geo[\"lon_q\"] = pd.qcut(\n",
    "        stations_geo[\"start_station_longitude\"], q=4, labels=[\"A\", \"B\", \"C\", \"D\"]\n",
    "    )\n",
    "\n",
    "    stations_geo[\"geo_zone\"] = stations_geo[\"lat_q\"].astype(str) + stations_geo[\"lon_q\"].astype(str)\n",
    "    zone_map = dict(zip(stations_geo[\"start_station_id\"].astype(int), stations_geo[\"geo_zone\"]))\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2) Simplify ALL transactions (NO sampling)\n",
    "    # -------------------------------------------------------\n",
    "    def simplify_transactions_full(transactions_list, zone_map):\n",
    "        simplified = []\n",
    "        for trans in transactions_list:\n",
    "            simple_trans = []\n",
    "            for item in trans:\n",
    "                if item in {\"morning_commute\", \"evening_commute\", \"non_commute\"}:\n",
    "                    simple_trans.append(item)\n",
    "                elif item in {\"good_weather\", \"poor_weather\"}:\n",
    "                    simple_trans.append(item)\n",
    "                elif item.startswith(\"start_\"):\n",
    "                    try:\n",
    "                        sid = int(item.split(\"_\")[1])\n",
    "                        z = zone_map.get(sid, \"UNK\")\n",
    "                        simple_trans.append(f\"start_zone_{z}\")\n",
    "                    except:\n",
    "                        pass\n",
    "                elif item.startswith(\"end_\"):\n",
    "                    try:\n",
    "                        sid = int(item.split(\"_\")[1])\n",
    "                        z = zone_map.get(sid, \"UNK\")\n",
    "                        simple_trans.append(f\"end_zone_{z}\")\n",
    "                    except:\n",
    "                        pass\n",
    "            if simple_trans:\n",
    "                simplified.append(simple_trans)\n",
    "        return simplified\n",
    "\n",
    "    print(\"Creating simplified transactions from ALL trips...\")\n",
    "    simplified_trans = simplify_transactions_full(all_transactions, zone_map)\n",
    "    print(f\"Transactions for mining: {len(simplified_trans):,} / {len(all_transactions):,}\")\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3) Sparse one-hot encoding\n",
    "    # -------------------------------------------------------\n",
    "    print(\"Sparse encoding transactions...\")\n",
    "    te = TransactionEncoder()\n",
    "    te_ary_sparse = te.fit(simplified_trans).transform(simplified_trans, sparse=True)\n",
    "\n",
    "    df_encoded = pd.DataFrame.sparse.from_spmatrix(te_ary_sparse, columns=te.columns_)\n",
    "    print(f\"Encoded matrix shape: {df_encoded.shape}\")\n",
    "    print(f\"Unique simplified items: {len(te.columns_)}\")\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 4) Mine frequent itemsets on FULL dataset\n",
    "    # -------------------------------------------------------\n",
    "    MIN_SUPPORT = 0.01\n",
    "    MAX_LEN = 3  \n",
    "\n",
    "    try:\n",
    "        print(\"Running Apriori (sparse, full data)...\")\n",
    "        frequent_itemsets = apriori(\n",
    "            df_encoded,\n",
    "            min_support=MIN_SUPPORT,\n",
    "            use_colnames=True,\n",
    "            max_len=MAX_LEN,\n",
    "            low_memory=True\n",
    "        )\n",
    "        algo_used = \"apriori\"\n",
    "    except MemoryError:\n",
    "        print(\"⚠️ Apriori OOM. Falling back to FP-Growth...\")\n",
    "        frequent_itemsets = fpgrowth(\n",
    "            df_encoded,\n",
    "            min_support=MIN_SUPPORT,\n",
    "            use_colnames=True,\n",
    "            max_len=MAX_LEN\n",
    "        )\n",
    "        algo_used = \"fpgrowth\"\n",
    "\n",
    "    print(f\"Frequent patterns found ({algo_used}): {len(frequent_itemsets)}\")\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 5) Association rules\n",
    "    # -------------------------------------------------------\n",
    "    if len(frequent_itemsets) > 1:\n",
    "        rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.1)\n",
    "        rules = rules.sort_values(\"lift\", ascending=False)\n",
    "        print(f\"Association rules generated: {len(rules)}\")\n",
    "    else:\n",
    "        rules = pd.DataFrame()\n",
    "        print(\"Not enough frequent patterns for association rules.\")\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 6) Full-dataset descriptive pattern counts\n",
    "    # -------------------------------------------------------\n",
    "    commute_patterns = {\"morning\": 0, \"evening\": 0, \"non_commute\": 0}\n",
    "    weather_patterns = {\"good\": 0, \"poor\": 0}\n",
    "\n",
    "    for trans in all_transactions:\n",
    "        s = set(trans)\n",
    "        if \"morning_commute\" in s:\n",
    "            commute_patterns[\"morning\"] += 1\n",
    "        elif \"evening_commute\" in s:\n",
    "            commute_patterns[\"evening\"] += 1\n",
    "        else:\n",
    "            commute_patterns[\"non_commute\"] += 1\n",
    "\n",
    "        if \"good_weather\" in s:\n",
    "            weather_patterns[\"good\"] += 1\n",
    "        else:\n",
    "            weather_patterns[\"poor\"] += 1\n",
    "\n",
    "    total_trans = max(len(all_transactions), 1)\n",
    "\n",
    "    print(\"\\nCommute Patterns:\")\n",
    "    print(f\"  Morning rush: {commute_patterns['morning']:,} ({commute_patterns['morning']/total_trans*100:.1f}%)\")\n",
    "    print(f\"  Evening rush: {commute_patterns['evening']:,} ({commute_patterns['evening']/total_trans*100:.1f}%)\")\n",
    "    print(f\"  Non-commute : {commute_patterns['non_commute']:,} ({commute_patterns['non_commute']/total_trans*100:.1f}%)\")\n",
    "\n",
    "    print(\"\\nWeather Impact:\")\n",
    "    print(f\"  Good weather trips: {weather_patterns['good']:,} ({weather_patterns['good']/total_trans*100:.1f}%)\")\n",
    "    print(f\"  Poor weather trips: {weather_patterns['poor']:,} ({weather_patterns['poor']/total_trans*100:.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"Mining results already exist\")\n",
    "    total_trans = max(len(all_transactions), 1)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# B) VISUALIZATIONS\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE PATTERN ANALYSIS WITH VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# GRAPH 1: TOP STATION ROUTES\n",
    "# -----------------------------------------------------\n",
    "print(\"\\n--- GRAPH 1: TOP STATION-TO-STATION ROUTES ---\")\n",
    "top_15_routes = sorted(route_counter.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "route_labels = [r.replace(\"_to_\", \" → \") for r, _ in top_15_routes]\n",
    "route_counts = [c for _, c in top_15_routes]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(route_labels)), route_counts)\n",
    "plt.yticks(range(len(route_labels)), route_labels, fontsize=10)\n",
    "plt.xlabel(\"Number of trips\")\n",
    "plt.title(\"Top observed station-to-station routes\", fontweight=\"bold\")\n",
    "for i, count in enumerate(route_counts):\n",
    "    plt.text(count + max(route_counts)*0.01, i, f\"{count:,}\", va=\"center\", fontsize=9)\n",
    "plt.grid(True, alpha=0.3, axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# GRAPH 2: BIDIRECTIONAL PAIRS\n",
    "# -----------------------------------------------------\n",
    "print(\"\\n--- GRAPH 2: BIDIRECTIONAL ROUTE PAIRS ---\")\n",
    "bidirectional_pairs = {}\n",
    "for route, count in route_counter.items():\n",
    "    s1, s2 = route.split(\"_to_\")\n",
    "    key = tuple(sorted([s1, s2]))\n",
    "    if key not in bidirectional_pairs:\n",
    "        bidirectional_pairs[key] = {\"forward\": 0, \"backward\": 0, \"total\": 0}\n",
    "    if (s1, s2) == key:\n",
    "        bidirectional_pairs[key][\"forward\"] += count\n",
    "    else:\n",
    "        bidirectional_pairs[key][\"backward\"] += count\n",
    "    bidirectional_pairs[key][\"total\"] += count\n",
    "\n",
    "top_pairs = sorted(bidirectional_pairs.items(), key=lambda x: x[1][\"total\"], reverse=True)[:10]\n",
    "\n",
    "pair_labels     = [f\"{s1}↔{s2}\" for (s1, s2), _ in top_pairs]\n",
    "forward_counts  = [c[\"forward\"] for _, c in top_pairs]\n",
    "backward_counts = [c[\"backward\"] for _, c in top_pairs]\n",
    "\n",
    "x = np.arange(len(pair_labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(x - width/2, forward_counts, width, label=\"Forward\", alpha=0.8)\n",
    "plt.bar(x + width/2, backward_counts, width, label=\"Backward\", alpha=0.8)\n",
    "plt.xticks(x, pair_labels, rotation=45, ha=\"right\", fontsize=9)\n",
    "plt.ylabel(\"Trips\")\n",
    "plt.title(\"Bidirectional Route Analysis - Rebalancing Needs\", fontweight=\"bold\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# GRAPH 3: GEO ZONE FLOW HEATMAP\n",
    "# -----------------------------------------------------\n",
    "print(\"\\n--- GRAPH 3: ZONE-TO-ZONE MOVEMENT PATTERNS ---\")\n",
    "zone_connections = {}\n",
    "for trans in simplified_trans:\n",
    "    start_zones = [item for item in trans if item.startswith(\"start_zone_\")]\n",
    "    end_zones   = [item for item in trans if item.startswith(\"end_zone_\")]\n",
    "    for start in start_zones:\n",
    "        for end in end_zones:\n",
    "            pair = f\"{start} → {end}\"\n",
    "            zone_connections[pair] = zone_connections.get(pair, 0) + 1\n",
    "\n",
    "start_zone_vals = sorted({z for t in simplified_trans for z in t if z.startswith(\"start_zone_\")})\n",
    "end_zone_vals   = sorted({z for t in simplified_trans for z in t if z.startswith(\"end_zone_\")})\n",
    "\n",
    "matrix = np.zeros((len(start_zone_vals), len(end_zone_vals)))\n",
    "for pair, count in zone_connections.items():\n",
    "    start, end = pair.split(\" → \")\n",
    "    i = start_zone_vals.index(start)\n",
    "    j = end_zone_vals.index(end)\n",
    "    matrix[i, j] = count\n",
    "\n",
    "matrix_pct = (matrix / matrix.sum()) * 100 if matrix.sum() > 0 else matrix\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "im = plt.imshow(matrix_pct, cmap=\"YlOrRd\", aspect=\"auto\")\n",
    "plt.colorbar(im, label=\"% of Trips\")\n",
    "plt.xticks(range(len(end_zone_vals)), end_zone_vals, rotation=45, ha=\"right\")\n",
    "plt.yticks(range(len(start_zone_vals)), start_zone_vals)\n",
    "plt.title(\"Geo Zone-to-Zone Movement Heatmap\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# GRAPH 4: ASSOCIATION RULES (TOP LIFT)\n",
    "# -----------------------------------------------------\n",
    "print(\"\\n--- GRAPH 4: ASSOCIATION RULE PAIRS ---\")\n",
    "if len(rules) > 0:\n",
    "    top_rules = rules.nlargest(10, \"lift\")\n",
    "    rule_labels = [\n",
    "        f'{\" & \".join(list(r[\"antecedents\"]))[:22]}… → {\" & \".join(list(r[\"consequents\"]))[:22]}…'\n",
    "        for _, r in top_rules.iterrows()\n",
    "    ]\n",
    "    lifts = top_rules[\"lift\"].values\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(range(len(rule_labels)), lifts)\n",
    "    plt.yticks(range(len(rule_labels)), rule_labels, fontsize=9)\n",
    "    plt.xlabel(\"Lift\")\n",
    "    plt.title(\"Top Association Rules by Lift\", fontweight=\"bold\")\n",
    "    plt.axvline(1, linestyle=\"--\", alpha=0.6)\n",
    "    plt.grid(True, alpha=0.3, axis=\"x\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No association rules found.\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# GRAPH 5: COMMUTE DISTRIBUTION\n",
    "# -----------------------------------------------------\n",
    "print(\"\\n--- GRAPH 5: COMMUTE PATTERN DISTRIBUTION ---\")\n",
    "categories = [\"Morning Rush\\n(7–9 AM)\", \"Evening Rush\\n(5–7 PM)\", \"Non-Commute\\n(Other Hours)\"]\n",
    "values = [commute_patterns[\"morning\"], commute_patterns[\"evening\"], commute_patterns[\"non_commute\"]]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "bars = plt.bar(range(3), values, alpha=0.85, edgecolor=\"black\", linewidth=1.2)\n",
    "plt.xticks(range(3), categories)\n",
    "plt.ylabel(\"Trips\")\n",
    "plt.title(\"Commute Pattern Distribution\", fontweight=\"bold\")\n",
    "plt.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "for bar, value in zip(bars, values):\n",
    "    pct = value / total_trans * 100\n",
    "    plt.text(bar.get_x() + bar.get_width()/2,\n",
    "             bar.get_height() + max(values)*0.01,\n",
    "             f\"{value:,}\\n({pct:.1f}%)\",\n",
    "             ha=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# FINAL SUMMARY\n",
    "# -----------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: ALL DISCOVERED PAIRS AND PATTERNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"1. Station Pairs:       {len(route_counter):,}\")\n",
    "print(f\"2. Bidirectional Pairs: {len(bidirectional_pairs):,}\")\n",
    "print(f\"3. Zone Connections:    {len(zone_connections):,}\")\n",
    "print(f\"4. Association Rules:   {len(rules):,}\")\n",
    "print(f\"5. Frequent Itemsets:   {len(frequent_itemsets):,}\")\n",
    "\n",
    "rush_pct = (commute_patterns[\"morning\"] + commute_patterns[\"evening\"]) / total_trans * 100\n",
    "top_route_pct = (top_15_routes[0][1] / total_trans * 100) if top_15_routes else 0\n",
    "imbalanced_pairs = sum(\n",
    "    1 for (_, c) in top_pairs\n",
    "    if c[\"total\"] > 0 and abs(c[\"forward\"] - c[\"backward\"]) / c[\"total\"] > 0.3\n",
    ")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"- Rush hours account for {rush_pct:.1f}% of trips\")\n",
    "print(f\"- Top route carries {top_route_pct:.2f}% of total traffic\")\n",
    "print(f\"- Rebalancing needed for {imbalanced_pairs} of top 10 bidirectional pairs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab9d80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:17:44.688065Z",
     "iopub.status.busy": "2025-11-27T20:17:44.687870Z",
     "iopub.status.idle": "2025-11-27T20:17:44.703021Z",
     "shell.execute_reply": "2025-11-27T20:17:44.702257Z",
     "shell.execute_reply.started": "2025-11-27T20:17:44.688049Z"
    }
   },
   "outputs": [],
   "source": [
    "dfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff2746",
   "metadata": {},
   "source": [
    "## Finalize User Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe94bf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:17:44.703691Z",
     "iopub.status.busy": "2025-11-27T20:17:44.703510Z",
     "iopub.status.idle": "2025-11-27T20:17:44.729928Z",
     "shell.execute_reply": "2025-11-27T20:17:44.729227Z",
     "shell.execute_reply.started": "2025-11-27T20:17:44.703675Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"USER CLUSTERING - AGGREGATING PROFILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Combine all batch profiles\n",
    "all_profiles = pd.concat(user_profiles_list, ignore_index=True)\n",
    "\n",
    "# Final aggregation\n",
    "user_profiles = all_profiles.groupby(['birth_year', 'gender']).agg({\n",
    "    ('tripduration', 'mean'): 'mean',\n",
    "    ('tripduration', 'std'): 'mean',\n",
    "    ('tripduration', 'count'): 'sum',\n",
    "    ('trip_duration_min', 'mean'): 'mean',\n",
    "    ('trip_duration_min', 'median'): 'mean',\n",
    "    ('start_station_id', 'nunique'): 'max',\n",
    "    ('end_station_id', 'nunique'): 'max',\n",
    "    ('hour', 'mean'): 'mean',\n",
    "    ('hour', 'std'): 'mean',\n",
    "    ('is_weekend', 'mean'): 'mean',\n",
    "    ('day_of_week', '<lambda>'): 'mean',\n",
    "    ('cycling_score', 'mean'): 'mean',\n",
    "    ('cycling_score', 'std'): 'mean',\n",
    "    ('temp_celsius', 'mean'): 'mean',\n",
    "    ('is_dry', 'mean'): 'mean',\n",
    "    ('usertype', '<lambda>'): 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns\n",
    "user_profiles.columns = [\n",
    "    'birth_year', 'gender', 'avg_duration', 'std_duration', 'trip_count',\n",
    "    'avg_trip_min', 'median_trip_min', 'unique_start_stations', 'unique_end_stations',\n",
    "    'avg_hour', 'std_hour', 'weekend_ratio', 'preferred_day',\n",
    "    'avg_cycling_score', 'std_cycling_score', 'avg_temp_preference', \n",
    "    'dry_weather_ratio', 'is_subscriber'\n",
    "]\n",
    "\n",
    "# Add derived features\n",
    "user_profiles['age'] = 2018 - user_profiles['birth_year']\n",
    "user_profiles['station_diversity'] = (user_profiles['unique_start_stations'] + \n",
    "                                      user_profiles['unique_end_stations']) / 2\n",
    "user_profiles['trips_per_month'] = user_profiles['trip_count'] / 3\n",
    "\n",
    "# Clean data\n",
    "user_profiles_clean = user_profiles.dropna()\n",
    "print(f\"User profiles created: {len(user_profiles_clean)} unique user groups\")\n",
    "print(f\"From {user_profiles_clean['trip_count'].sum():.0f} total trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db43fe",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176e295",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:17:44.730752Z",
     "iopub.status.busy": "2025-11-27T20:17:44.730570Z",
     "iopub.status.idle": "2025-11-27T20:17:44.783021Z",
     "shell.execute_reply": "2025-11-27T20:17:44.782057Z",
     "shell.execute_reply.started": "2025-11-27T20:17:44.730737Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HIERARCHICAL CLUSTERING ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select features\n",
    "clustering_features = [\n",
    "    'age', 'avg_duration', 'trip_count', 'station_diversity',\n",
    "    'avg_trip_min', 'weekend_ratio', 'avg_hour',\n",
    "    'avg_cycling_score', 'dry_weather_ratio', 'trips_per_month'\n",
    "]\n",
    "\n",
    "available_features = [col for col in clustering_features if col in user_profiles_clean.columns]\n",
    "X = user_profiles_clean[available_features].values\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(f\"Clustering matrix: {X_scaled.shape}\")\n",
    "\n",
    "# Find best linkage method\n",
    "linkage_methods = ['ward', 'average', 'complete']\n",
    "best_method = None\n",
    "best_score = float('inf')\n",
    "\n",
    "for method in linkage_methods:\n",
    "    linkage_test = linkage(X_scaled, method=method)\n",
    "    clusters_test = fcluster(linkage_test, 4, criterion='maxclust')\n",
    "    db_score = davies_bouldin_score(X_scaled, clusters_test)\n",
    "    print(f\"  {method}: Davies-Bouldin = {db_score:.3f}\")\n",
    "    if db_score < best_score:\n",
    "        best_score = db_score\n",
    "        best_method = method\n",
    "\n",
    "print(f\"\\nBest method: {best_method}\")\n",
    "\n",
    "# Perform clustering\n",
    "linkage_matrix = linkage(X_scaled, method=best_method)\n",
    "\n",
    "# Find optimal k\n",
    "db_scores = []\n",
    "sil_scores = []\n",
    "k_range = range(3, 9)\n",
    "\n",
    "for k in k_range:\n",
    "    clusters = fcluster(linkage_matrix, k, criterion='maxclust')\n",
    "    db = davies_bouldin_score(X_scaled, clusters)\n",
    "    sil = silhouette_score(X_scaled, clusters)\n",
    "    db_scores.append(db)\n",
    "    sil_scores.append(sil)\n",
    "    print(f\"  k={k}: DB={db:.3f}, Silhouette={sil:.3f}\")\n",
    "\n",
    "optimal_k = k_range[np.argmin(db_scores)]\n",
    "print(f\"\\nOptimal clusters: {optimal_k}\")\n",
    "\n",
    "# Final clustering\n",
    "final_clusters = fcluster(linkage_matrix, optimal_k, criterion='maxclust')\n",
    "user_profiles_clean['cluster'] = final_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787bb092",
   "metadata": {},
   "source": [
    "# Visualize Clustering Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e60e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:17:44.783773Z",
     "iopub.status.busy": "2025-11-27T20:17:44.783584Z",
     "iopub.status.idle": "2025-11-27T20:17:45.975718Z",
     "shell.execute_reply": "2025-11-27T20:17:45.974988Z",
     "shell.execute_reply.started": "2025-11-27T20:17:44.783758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add these imports at the top of your notebook\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Then run your visualization code\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Dendrogram\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "dendrogram(linkage_matrix, truncate_mode='lastp', p=15, no_labels=True, ax=ax1)\n",
    "ax1.axhline(y=linkage_matrix[-(optimal_k-1), 2], color='r', linestyle='--', alpha=0.7)\n",
    "ax1.set_title('Hierarchical Clustering Dendrogram', fontweight='bold')\n",
    "ax1.set_xlabel('User Sample Index')\n",
    "ax1.set_ylabel('Distance')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "k_range_plot = list(range(3, 3 + len(db_scores)))\n",
    "ax2.plot(k_range_plot, db_scores, 'b-o', label='Davies-Bouldin')\n",
    "ax2_twin = ax2.twinx()\n",
    "ax2_twin.plot(k_range_plot, sil_scores, 'r-s', label='Silhouette')\n",
    "ax2.set_xlabel('Number of Clusters')\n",
    "ax2.set_ylabel('Davies-Bouldin Score', color='b')\n",
    "ax2_twin.set_ylabel('Silhouette Score', color='r')\n",
    "ax2.axvline(x=optimal_k, color='green', linestyle=':', alpha=0.7)\n",
    "ax2.set_title('Cluster Evaluation Metrics', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# PCA visualization\n",
    "ax3 = plt.subplot(2, 2, 3)\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "scatter = ax3.scatter(X_pca[:, 0], X_pca[:, 1], c=final_clusters, cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(scatter, ax=ax3, label='Cluster')\n",
    "ax3.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} var)')\n",
    "ax3.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} var)')\n",
    "ax3.set_title('Clusters in PCA Space', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution\n",
    "ax4 = plt.subplot(2, 2, 4)\n",
    "cluster_sizes = user_profiles_clean['cluster'].value_counts().sort_index()\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(cluster_sizes)))\n",
    "bars = ax4.bar(cluster_sizes.index, cluster_sizes.values, color=colors)\n",
    "ax4.set_xlabel('Cluster')\n",
    "ax4.set_ylabel('Number of Users')\n",
    "ax4.set_title('User Distribution', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, count in zip(bars, cluster_sizes.values):\n",
    "    pct = count / len(user_profiles_clean) * 100\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "            f'{count}\\n({pct:.1f}%)', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0516b4e",
   "metadata": {},
   "source": [
    "# Analyze Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24401057",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:17:45.976597Z",
     "iopub.status.busy": "2025-11-27T20:17:45.976418Z",
     "iopub.status.idle": "2025-11-27T20:17:45.989199Z",
     "shell.execute_reply": "2025-11-27T20:17:45.988465Z",
     "shell.execute_reply.started": "2025-11-27T20:17:45.976580Z"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze each cluster\n",
    "for cluster_id in range(1, optimal_k + 1):\n",
    "    cluster = user_profiles_clean[user_profiles_clean['cluster'] == cluster_id]\n",
    "    \n",
    "    print(f\"\\nCLUSTER {cluster_id}: {len(cluster)} users ({len(cluster)/len(user_profiles_clean)*100:.1f}%)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Age: {cluster['age'].mean():.1f} years\")\n",
    "    print(f\"Trips/month: {cluster['trips_per_month'].mean():.1f}\")\n",
    "    print(f\"Trip duration: {cluster['avg_trip_min'].mean():.1f} min\")\n",
    "    print(f\"Weekend usage: {cluster['weekend_ratio'].mean():.1%}\")\n",
    "    print(f\"Station diversity: {cluster['station_diversity'].mean():.1f}\")\n",
    "    print(f\"Weather score: {cluster['avg_cycling_score'].mean():.1f}/100\")\n",
    "\n",
    "# Summary table\n",
    "cluster_summary = user_profiles_clean.groupby('cluster').agg({\n",
    "    'age': 'mean',\n",
    "    'trip_count': 'mean',\n",
    "    'avg_trip_min': 'mean',\n",
    "    'weekend_ratio': 'mean',\n",
    "    'station_diversity': 'mean',\n",
    "    'avg_cycling_score': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLUSTER SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(cluster_summary)\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0354fc",
   "metadata": {},
   "source": [
    "# Temporal Demand Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0a9bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:17:45.989980Z",
     "iopub.status.busy": "2025-11-27T20:17:45.989809Z",
     "iopub.status.idle": "2025-11-27T20:17:53.614302Z",
     "shell.execute_reply": "2025-11-27T20:17:53.613361Z",
     "shell.execute_reply.started": "2025-11-27T20:17:45.989964Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEMPORAL DEMAND ANALYSIS & PATTERNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Hourly demand patterns by user type\n",
    "hourly_patterns = dfm.groupby([dfm['start_hour'].dt.hour, 'usertype']).agg({\n",
    "    'start_station_id': 'count',\n",
    "    'trip_duration_min': 'mean',\n",
    "    'cycling_score': 'mean'\n",
    "}).rename(columns={'start_station_id': 'trips'}).reset_index()\n",
    "\n",
    "# Peak vs Off-peak analysis\n",
    "peak_hours = [7, 8, 9, 17, 18, 19]\n",
    "dfm['is_peak'] = dfm['hour'].isin(peak_hours).astype(int)\n",
    "\n",
    "peak_analysis = dfm.groupby('is_peak').agg({\n",
    "    'trip_duration_min': ['mean', 'std'],\n",
    "    'start_station_id': 'count',\n",
    "    'usertype': lambda x: (x == 'Subscriber').mean()\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nPeak vs Off-Peak Analysis:\")\n",
    "print(peak_analysis)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Hourly pattern by user type\n",
    "pivot_hourly = hourly_patterns.pivot(index='start_hour', columns='usertype', values='trips')\n",
    "pivot_hourly.plot(ax=axes[0,0], marker='o')\n",
    "axes[0,0].set_title('Hourly Demand by User Type', fontweight='bold')\n",
    "axes[0,0].set_xlabel('Hour of Day')\n",
    "axes[0,0].set_ylabel('Number of Trips')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Weekly pattern heatmap\n",
    "weekly_hourly = dfm.groupby(['day_of_week', 'hour']).size().reset_index(name='trips')\n",
    "weekly_pivot = weekly_hourly.pivot(index='hour', columns='day_of_week', values='trips')\n",
    "sns.heatmap(weekly_pivot, cmap='YlOrRd', ax=axes[0,1], cbar_kws={'label': 'Trips'})\n",
    "axes[0,1].set_title('Weekly-Hourly Demand Heatmap', fontweight='bold')\n",
    "axes[0,1].set_xlabel('Day of Week')\n",
    "axes[0,1].set_ylabel('Hour of Day')\n",
    "\n",
    "# Monthly trend\n",
    "monthly_trend = dfm.groupby('month').agg({\n",
    "    'start_station_id': 'count',\n",
    "    'cycling_score': 'mean',\n",
    "    'temp_celsius': 'mean'\n",
    "}).rename(columns={'start_station_id': 'trips'})\n",
    "\n",
    "ax1 = axes[1,0]\n",
    "ax1.bar(monthly_trend.index, monthly_trend['trips'], color='steelblue', alpha=0.7)\n",
    "ax1.set_xlabel('Month')\n",
    "ax1.set_ylabel('Number of Trips', color='steelblue')\n",
    "ax1.set_title('Monthly Trends with Weather', fontweight='bold')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(monthly_trend.index, monthly_trend['temp_celsius'], 'r-o', label='Temperature')\n",
    "ax2.set_ylabel('Temperature (°C)', color='red')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Duration distribution\n",
    "axes[1,1].hist(dfm['trip_duration_min'].clip(0, 60), bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1,1].axvline(dfm['trip_duration_min'].median(), color='red', linestyle='--', label=f'Median: {dfm[\"trip_duration_min\"].median():.1f} min')\n",
    "axes[1,1].set_xlabel('Trip Duration (minutes)')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "axes[1,1].set_title('Trip Duration Distribution', fontweight='bold')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c8e9c2",
   "metadata": {},
   "source": [
    "# Weather Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e814542",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PEARSON CORRELATION: WEATHER vs TRIP DEMAND\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Aggregate hourly trip counts with weather variables\n",
    "hourly_data = dfm.groupby('start_hour').agg({\n",
    "    'start_station_id': 'count',  # trip count\n",
    "    'temp_celsius': 'mean',\n",
    "    'apparent_temperature': 'mean',\n",
    "    'rain': 'mean',\n",
    "    'snowfall': 'mean',\n",
    "    'wind_kmh': 'mean',\n",
    "    'relative_humidity_2m': 'mean',\n",
    "    'cloud_cover': 'mean',\n",
    "    'visibility_km': 'mean',\n",
    "    'cycling_score': 'mean',\n",
    "    'is_dry': 'mean'\n",
    "}).rename(columns={'start_station_id': 'trip_count'}).reset_index()\n",
    "\n",
    "# Calculate Pearson correlation with p-values\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "weather_vars = [\n",
    "    'temp_celsius', 'apparent_temperature', 'rain', 'snowfall', \n",
    "    'wind_kmh', 'relative_humidity_2m', 'cloud_cover', \n",
    "    'visibility_km', 'cycling_score', 'is_dry'\n",
    "]\n",
    "\n",
    "correlations = {}\n",
    "p_values = {}\n",
    "\n",
    "for var in weather_vars:\n",
    "    if var in hourly_data.columns:\n",
    "        # Remove NaN values for correlation calculation\n",
    "        valid_mask = hourly_data[['trip_count', var]].notna().all(axis=1)\n",
    "        if valid_mask.sum() > 2:  # Need at least 3 points for correlation\n",
    "            corr, p_val = pearsonr(\n",
    "                hourly_data.loc[valid_mask, 'trip_count'], \n",
    "                hourly_data.loc[valid_mask, var]\n",
    "            )\n",
    "            correlations[var] = corr\n",
    "            p_values[var] = p_val\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "corr_df = pd.DataFrame({\n",
    "    'Pearson Correlation': correlations,\n",
    "    'p-value': p_values\n",
    "})\n",
    "corr_df['Significant'] = corr_df['p-value'] < 0.05\n",
    "corr_df = corr_df.sort_values('Pearson Correlation', ascending=False)\n",
    "\n",
    "print(\"\\nPearson Correlation Coefficients (Weather vs Trip Count):\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'Variable':<25} {'Correlation':>12} {'p-value':>12} {'Significance':>15} {'Strength':>15}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for var, row in corr_df.iterrows():\n",
    "    corr_value = row['Pearson Correlation']\n",
    "    p_val = row['p-value']\n",
    "    strength = \"Strong\" if abs(corr_value) > 0.5 else \"Moderate\" if abs(corr_value) > 0.3 else \"Weak\"\n",
    "    direction = \"positive\" if corr_value > 0 else \"negative\"\n",
    "    sig_marker = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"n.s.\"\n",
    "    \n",
    "    print(f\"{var:<25} {corr_value:>12.4f} {p_val:>12.2e} {sig_marker:>15} {strength + ' ' + direction:>15}\")\n",
    "\n",
    "print(\"\\nSignificance levels: *** p<0.001, ** p<0.01, * p<0.05, n.s. = not significant\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Correlation bar chart\n",
    "ax1 = axes[0, 0]\n",
    "colors = ['green' if x > 0 else 'red' for x in corr_df['Pearson Correlation']]\n",
    "corr_df.plot(kind='barh', ax=ax1, legend=False, color=colors, alpha=0.7)\n",
    "ax1.set_xlabel('Pearson Correlation Coefficient')\n",
    "ax1.set_title('Weather Variables vs Trip Count Correlation', fontweight='bold')\n",
    "ax1.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Temperature vs Trip Count scatter\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(hourly_data['temp_celsius'], hourly_data['trip_count'], alpha=0.5, s=20)\n",
    "z = np.polyfit(hourly_data['temp_celsius'].dropna(), \n",
    "               hourly_data.loc[hourly_data['temp_celsius'].notna(), 'trip_count'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax2.plot(hourly_data['temp_celsius'].sort_values(), \n",
    "         p(hourly_data['temp_celsius'].sort_values()), \n",
    "         \"r--\", alpha=0.8, linewidth=2, \n",
    "         label=f'r = {correlations[\"temp_celsius\"]:.3f}')\n",
    "ax2.set_xlabel('Temperature (°C)')\n",
    "ax2.set_ylabel('Trip Count')\n",
    "ax2.set_title('Temperature vs Trip Demand', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cycling Score vs Trip Count\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(hourly_data['cycling_score'], hourly_data['trip_count'], alpha=0.5, s=20, color='green')\n",
    "z = np.polyfit(hourly_data['cycling_score'].dropna(), \n",
    "               hourly_data.loc[hourly_data['cycling_score'].notna(), 'trip_count'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax3.plot(hourly_data['cycling_score'].sort_values(), \n",
    "         p(hourly_data['cycling_score'].sort_values()), \n",
    "         \"r--\", alpha=0.8, linewidth=2, \n",
    "         label=f'r = {correlations[\"cycling_score\"]:.3f}')\n",
    "ax3.set_xlabel('Cycling Score (0-100)')\n",
    "ax3.set_ylabel('Trip Count')\n",
    "ax3.set_title('Cycling Score vs Trip Demand', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Correlation heatmap\n",
    "ax4 = axes[1, 1]\n",
    "corr_matrix = hourly_data[['trip_count'] + weather_vars].corr()\n",
    "trip_corr = corr_matrix[['trip_count']].drop('trip_count')\n",
    "sns.heatmap(trip_corr, annot=True, fmt='.3f', cmap='RdYlGn', center=0, \n",
    "            ax=ax4, cbar_kws={'label': 'Correlation'}, vmin=-1, vmax=1)\n",
    "ax4.set_title('Weather Correlation Heatmap', fontweight='bold')\n",
    "ax4.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key insights\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "strongest_pos = corr_df[corr_df['Pearson Correlation'] > 0].iloc[0]\n",
    "strongest_neg = corr_df[corr_df['Pearson Correlation'] < 0].iloc[-1] if any(corr_df['Pearson Correlation'] < 0) else None\n",
    "\n",
    "print(f\"✓ Strongest positive correlation: {strongest_pos.name} (r = {strongest_pos['Pearson Correlation']:.4f}, p = {strongest_pos['p-value']:.2e})\")\n",
    "if strongest_neg is not None:\n",
    "    print(f\"✓ Strongest negative correlation: {strongest_neg.name} (r = {strongest_neg['Pearson Correlation']:.4f}, p = {strongest_neg['p-value']:.2e})\")\n",
    "\n",
    "significant_vars = corr_df[corr_df['Significant'] == True]\n",
    "moderate_strong = corr_df[abs(corr_df['Pearson Correlation']) > 0.3]\n",
    "\n",
    "print(f\"✓ {len(significant_vars)} variables are statistically significant (p < 0.05)\")\n",
    "print(f\"✓ {len(moderate_strong)} variables show moderate-to-strong correlation (|r| > 0.3)\")\n",
    "print(f\"✓ Weather (cycling score) explains approximately {(correlations.get('cycling_score', 0)**2)*100:.1f}% of trip demand variance (R² = {correlations.get('cycling_score', 0)**2:.3f})\")\n",
    "\n",
    "# Count highly significant correlations\n",
    "highly_sig = corr_df[corr_df['p-value'] < 0.001]\n",
    "print(f\"✓ {len(highly_sig)} variables are highly significant (p < 0.001), indicating robust relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a56ee3",
   "metadata": {},
   "source": [
    "# Export All Results to PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cbf812",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib reportlab fpdf pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPORTING ALL RESULTS TO PDF\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create output filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "pdf_filename = f\"Bike_Sharing_Analysis_Results_{timestamp}.pdf\"\n",
    "\n",
    "# Create PDF with all results\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    \n",
    "    # ===== PAGE 1: SUMMARY STATISTICS =====\n",
    "    fig = plt.figure(figsize=(11, 8.5))\n",
    "    fig.suptitle('Bike Sharing Analysis - Summary Statistics', fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "DATASET OVERVIEW\n",
    "{'='*80}\n",
    "Total Trips: {len(dfm):,}\n",
    "Date Range: {START} → {END}\n",
    "Total Users: {len(user_profiles_clean):,} unique user groups\n",
    "    \n",
    "PATTERN MINING RESULTS\n",
    "{'='*80}\n",
    "Station Pairs: {len(route_counter):,}\n",
    "Bidirectional Pairs: {len(bidirectional_pairs):,}\n",
    "Zone Connections: {len(zone_connections):,}\n",
    "Association Rules: {len(rules):,}\n",
    "Frequent Itemsets: {len(frequent_itemsets):,}\n",
    "\n",
    "TOP 10 ROUTES\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "    for i, (route, count) in enumerate(top_10_routes, 1):\n",
    "        s1, s2 = route.split(\"_to_\")\n",
    "        pct = count / total_trans * 100\n",
    "        summary_text += f\"{i:2}. {s1} → {s2}: {count:,} trips ({pct:.2f}%)\\n\"\n",
    "    \n",
    "    summary_text += f\"\"\"\n",
    "COMMUTE PATTERNS\n",
    "{'='*80}\n",
    "Morning Rush (7-9 AM): {commute_patterns['morning']:,} ({commute_patterns['morning']/total_trans*100:.1f}%)\n",
    "Evening Rush (5-7 PM): {commute_patterns['evening']:,} ({commute_patterns['evening']/total_trans*100:.1f}%)\n",
    "Non-Commute Hours: {commute_patterns['non_commute']:,} ({commute_patterns['non_commute']/total_trans*100:.1f}%)\n",
    "\n",
    "WEATHER IMPACT\n",
    "{'='*80}\n",
    "Good Weather Trips: {weather_patterns['good']:,} ({weather_patterns['good']/total_trans*100:.1f}%)\n",
    "Poor Weather Trips: {weather_patterns['poor']:,} ({weather_patterns['poor']/total_trans*100:.1f}%)\n",
    "\n",
    "CLUSTERING RESULTS\n",
    "{'='*80}\n",
    "Optimal Number of Clusters: {optimal_k}\n",
    "Clustering Method: {best_method}\n",
    "Davies-Bouldin Score: {best_score:.3f}\n",
    "\"\"\"\n",
    "    \n",
    "    plt.text(0.05, 0.95, summary_text, transform=fig.transFigure, \n",
    "             fontsize=9, verticalalignment='top', fontfamily='monospace')\n",
    "    plt.axis('off')\n",
    "    pdf.savefig(fig, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # ===== PAGE 2: CLUSTER SUMMARY TABLE =====\n",
    "    fig = plt.figure(figsize=(11, 8.5))\n",
    "    fig.suptitle('User Clustering - Detailed Summary', fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    cluster_text = \"CLUSTER CHARACTERISTICS\\n\" + \"=\"*80 + \"\\n\\n\"\n",
    "    for cluster_id in range(1, optimal_k + 1):\n",
    "        cluster = user_profiles_clean[user_profiles_clean['cluster'] == cluster_id]\n",
    "        cluster_text += f\"CLUSTER {cluster_id}: {len(cluster)} users ({len(cluster)/len(user_profiles_clean)*100:.1f}%)\\n\"\n",
    "        cluster_text += \"-\" * 50 + \"\\n\"\n",
    "        cluster_text += f\"Age: {cluster['age'].mean():.1f} years\\n\"\n",
    "        cluster_text += f\"Trips/month: {cluster['trips_per_month'].mean():.1f}\\n\"\n",
    "        cluster_text += f\"Trip duration: {cluster['avg_trip_min'].mean():.1f} min\\n\"\n",
    "        cluster_text += f\"Weekend usage: {cluster['weekend_ratio'].mean():.1%}\\n\"\n",
    "        cluster_text += f\"Station diversity: {cluster['station_diversity'].mean():.1f}\\n\"\n",
    "        cluster_text += f\"Weather score: {cluster['avg_cycling_score'].mean():.1f}/100\\n\\n\"\n",
    "    \n",
    "    cluster_text += \"\\nCLUSTER SUMMARY TABLE\\n\" + \"=\"*80 + \"\\n\"\n",
    "    cluster_text += cluster_summary.to_string()\n",
    "    \n",
    "    plt.text(0.05, 0.95, cluster_text, transform=fig.transFigure,\n",
    "             fontsize=9, verticalalignment='top', fontfamily='monospace')\n",
    "    plt.axis('off')\n",
    "    pdf.savefig(fig, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # ===== PAGE 3: WEATHER CORRELATION RESULTS =====\n",
    "    fig = plt.figure(figsize=(11, 8.5))\n",
    "    fig.suptitle('Weather Correlation Analysis', fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    corr_text = \"PEARSON CORRELATION: WEATHER vs TRIP DEMAND\\n\" + \"=\"*80 + \"\\n\\n\"\n",
    "    corr_text += f\"{'Variable':<25} {'Correlation':>12} {'p-value':>12} {'Significance':>15}\\n\"\n",
    "    corr_text += \"-\" * 80 + \"\\n\"\n",
    "    \n",
    "    for var, row in corr_df.iterrows():\n",
    "        corr_value = row['Pearson Correlation']\n",
    "        p_val = row['p-value']\n",
    "        sig_marker = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"n.s.\"\n",
    "        corr_text += f\"{var:<25} {corr_value:>12.4f} {p_val:>12.2e} {sig_marker:>15}\\n\"\n",
    "    \n",
    "    corr_text += f\"\\n\\nKEY INSIGHTS\\n{'='*80}\\n\"\n",
    "    strongest_pos = corr_df[corr_df['Pearson Correlation'] > 0].iloc[0]\n",
    "    corr_text += f\"✓ Strongest positive: {strongest_pos.name} (r={strongest_pos['Pearson Correlation']:.4f})\\n\"\n",
    "    corr_text += f\"✓ {len(significant_vars)} variables statistically significant (p < 0.05)\\n\"\n",
    "    corr_text += f\"✓ {len(moderate_strong)} variables show moderate-to-strong correlation (|r| > 0.3)\\n\"\n",
    "    corr_text += f\"✓ Cycling score explains {(correlations.get('cycling_score', 0)**2)*100:.1f}% of variance\\n\"\n",
    "    corr_text += f\"✓ {len(highly_sig)} variables highly significant (p < 0.001)\\n\"\n",
    "    \n",
    "    plt.text(0.05, 0.95, corr_text, transform=fig.transFigure,\n",
    "             fontsize=9, verticalalignment='top', fontfamily='monospace')\n",
    "    plt.axis('off')\n",
    "    pdf.savefig(fig, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # ===== SAVE ALL EXISTING FIGURES =====\n",
    "    # We'll need to regenerate the plots to save them\n",
    "    \n",
    "    # Top Routes\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    route_labels = [r.replace(\"_to_\", \" → \") for r, _ in top_15_routes]\n",
    "    route_counts = [c for _, c in top_15_routes]\n",
    "    plt.barh(range(len(route_labels)), route_counts)\n",
    "    plt.yticks(range(len(route_labels)), route_labels, fontsize=10)\n",
    "    plt.xlabel(\"Number of trips\")\n",
    "    plt.title(\"Top Station-to-Station Routes\", fontweight=\"bold\")\n",
    "    for i, count in enumerate(route_counts):\n",
    "        plt.text(count + max(route_counts)*0.01, i, f\"{count:,}\", va=\"center\", fontsize=9)\n",
    "    plt.grid(True, alpha=0.3, axis=\"x\")\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig)\n",
    "    plt.close()\n",
    "    \n",
    "    # Bidirectional Pairs\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    x = np.arange(len(pair_labels))\n",
    "    width = 0.35\n",
    "    plt.bar(x - width/2, forward_counts, width, label=\"Forward\", alpha=0.8)\n",
    "    plt.bar(x + width/2, backward_counts, width, label=\"Backward\", alpha=0.8)\n",
    "    plt.xticks(x, pair_labels, rotation=45, ha=\"right\", fontsize=9)\n",
    "    plt.ylabel(\"Trips\")\n",
    "    plt.title(\"Bidirectional Route Analysis - Rebalancing Needs\", fontweight=\"bold\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3, axis=\"y\")\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig)\n",
    "    plt.close()\n",
    "    \n",
    "    # Zone Flow Heatmap\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    im = plt.imshow(matrix_pct, cmap=\"YlOrRd\", aspect=\"auto\")\n",
    "    plt.colorbar(im, label=\"% of Trips\")\n",
    "    plt.xticks(range(len(end_zone_vals)), end_zone_vals, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(start_zone_vals)), start_zone_vals)\n",
    "    plt.title(\"Geo Zone-to-Zone Movement Heatmap\", fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig)\n",
    "    plt.close()\n",
    "    \n",
    "    # Association Rules\n",
    "    if len(rules) > 0:\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        top_rules = rules.nlargest(10, \"lift\")\n",
    "        rule_labels = [\n",
    "            f'{\" & \".join(list(r[\"antecedents\"]))[:22]}… → {\" & \".join(list(r[\"consequents\"]))[:22]}…'\n",
    "            for _, r in top_rules.iterrows()\n",
    "        ]\n",
    "        lifts = top_rules[\"lift\"].values\n",
    "        plt.barh(range(len(rule_labels)), lifts)\n",
    "        plt.yticks(range(len(rule_labels)), rule_labels, fontsize=9)\n",
    "        plt.xlabel(\"Lift\")\n",
    "        plt.title(\"Top Association Rules by Lift\", fontweight=\"bold\")\n",
    "        plt.axvline(1, linestyle=\"--\", alpha=0.6)\n",
    "        plt.grid(True, alpha=0.3, axis=\"x\")\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig)\n",
    "        plt.close()\n",
    "    \n",
    "    # Commute Distribution\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    categories = [\"Morning Rush\\n(7–9 AM)\", \"Evening Rush\\n(5–7 PM)\", \"Non-Commute\\n(Other Hours)\"]\n",
    "    values = [commute_patterns[\"morning\"], commute_patterns[\"evening\"], commute_patterns[\"non_commute\"]]\n",
    "    bars = plt.bar(range(3), values, alpha=0.85, edgecolor=\"black\", linewidth=1.2)\n",
    "    plt.xticks(range(3), categories)\n",
    "    plt.ylabel(\"Trips\")\n",
    "    plt.title(\"Commute Pattern Distribution\", fontweight=\"bold\")\n",
    "    plt.grid(True, alpha=0.3, axis=\"y\")\n",
    "    for bar, value in zip(bars, values):\n",
    "        pct = value / total_trans * 100\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,\n",
    "                f\"{value:,}\\n({pct:.1f}%)\", ha=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig)\n",
    "    plt.close()\n",
    "    \n",
    "    # Clustering Visualizations\n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Dendrogram\n",
    "    ax1 = plt.subplot(2, 2, 1)\n",
    "    dendrogram(linkage_matrix, truncate_mode='lastp', p=15, no_labels=True, ax=ax1)\n",
    "    ax1.axhline(y=linkage_matrix[-(optimal_k-1), 2], color='r', linestyle='--', alpha=0.7)\n",
    "    ax1.set_title('Hierarchical Clustering Dendrogram', fontweight='bold')\n",
    "    ax1.set_xlabel('User Sample Index')\n",
    "    ax1.set_ylabel('Distance')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Metrics\n",
    "    ax2 = plt.subplot(2, 2, 2)\n",
    "    k_range_plot = list(range(3, 3 + len(db_scores)))\n",
    "    ax2.plot(k_range_plot, db_scores, 'b-o', label='Davies-Bouldin')\n",
    "    ax2_twin = ax2.twinx()\n",
    "    ax2_twin.plot(k_range_plot, sil_scores, 'r-s', label='Silhouette')\n",
    "    ax2.set_xlabel('Number of Clusters')\n",
    "    ax2.set_ylabel('Davies-Bouldin Score', color='b')\n",
    "    ax2_twin.set_ylabel('Silhouette Score', color='r')\n",
    "    ax2.axvline(x=optimal_k, color='green', linestyle=':', alpha=0.7)\n",
    "    ax2.set_title('Cluster Evaluation Metrics', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # PCA\n",
    "    ax3 = plt.subplot(2, 2, 3)\n",
    "    scatter = ax3.scatter(X_pca[:, 0], X_pca[:, 1], c=final_clusters, cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(scatter, ax=ax3, label='Cluster')\n",
    "    ax3.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} var)')\n",
    "    ax3.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} var)')\n",
    "    ax3.set_title('Clusters in PCA Space', fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution\n",
    "    ax4 = plt.subplot(2, 2, 4)\n",
    "    cluster_sizes = user_profiles_clean['cluster'].value_counts().sort_index()\n",
    "    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(cluster_sizes)))\n",
    "    bars = ax4.bar(cluster_sizes.index, cluster_sizes.values, color=colors)\n",
    "    ax4.set_xlabel('Cluster')\n",
    "    ax4.set_ylabel('Number of Users')\n",
    "    ax4.set_title('User Distribution', fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    for bar, count in zip(bars, cluster_sizes.values):\n",
    "        pct = count / len(user_profiles_clean) * 100\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                f'{count}\\n({pct:.1f}%)', ha='center', va='bottom')\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig)\n",
    "    plt.close()\n",
    "    \n",
    "    # Temporal Demand Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Hourly pattern\n",
    "    pivot_hourly = hourly_patterns.pivot(index='start_hour', columns='usertype', values='trips')\n",
    "    pivot_hourly.plot(ax=axes[0,0], marker='o')\n",
    "    axes[0,0].set_title('Hourly Demand by User Type', fontweight='bold')\n",
    "    axes[0,0].set_xlabel('Hour of Day')\n",
    "    axes[0,0].set_ylabel('Number of Trips')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Weekly heatmap\n",
    "    weekly_hourly = dfm.groupby(['day_of_week', 'hour']).size().reset_index(name='trips')\n",
    "    weekly_pivot = weekly_hourly.pivot(index='hour', columns='day_of_week', values='trips')\n",
    "    sns.heatmap(weekly_pivot, cmap='YlOrRd', ax=axes[0,1], cbar_kws={'label': 'Trips'})\n",
    "    axes[0,1].set_title('Weekly-Hourly Demand Heatmap', fontweight='bold')\n",
    "    axes[0,1].set_xlabel('Day of Week')\n",
    "    axes[0,1].set_ylabel('Hour of Day')\n",
    "    \n",
    "    # Monthly trend\n",
    "    monthly_trend = dfm.groupby('month').agg({\n",
    "        'start_station_id': 'count',\n",
    "        'cycling_score': 'mean',\n",
    "        'temp_celsius': 'mean'\n",
    "    }).rename(columns={'start_station_id': 'trips'})\n",
    "    \n",
    "    ax1 = axes[1,0]\n",
    "    ax1.bar(monthly_trend.index, monthly_trend['trips'], color='steelblue', alpha=0.7)\n",
    "    ax1.set_xlabel('Month')\n",
    "    ax1.set_ylabel('Number of Trips', color='steelblue')\n",
    "    ax1.set_title('Monthly Trends with Weather', fontweight='bold')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(monthly_trend.index, monthly_trend['temp_celsius'], 'r-o', label='Temperature')\n",
    "    ax2.set_ylabel('Temperature (°C)', color='red')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Duration distribution\n",
    "    axes[1,1].hist(dfm['trip_duration_min'].clip(0, 60), bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[1,1].axvline(dfm['trip_duration_min'].median(), color='red', linestyle='--', \n",
    "                     label=f'Median: {dfm[\"trip_duration_min\"].median():.1f} min')\n",
    "    axes[1,1].set_xlabel('Trip Duration (minutes)')\n",
    "    axes[1,1].set_ylabel('Frequency')\n",
    "    axes[1,1].set_title('Trip Duration Distribution', fontweight='bold')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig)\n",
    "    plt.close()\n",
    "    \n",
    "    # Weather Correlation Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Correlation bar chart\n",
    "    ax1 = axes[0, 0]\n",
    "    colors = ['green' if x > 0 else 'red' for x in corr_df['Pearson Correlation']]\n",
    "    corr_df['Pearson Correlation'].plot(kind='barh', ax=ax1, color=colors, alpha=0.7)\n",
    "    ax1.set_xlabel('Pearson Correlation Coefficient')\n",
    "    ax1.set_title('Weather Variables vs Trip Count Correlation', fontweight='bold')\n",
    "    ax1.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Temperature scatter\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.scatter(hourly_data['temp_celsius'], hourly_data['trip_count'], alpha=0.5, s=20)\n",
    "    z = np.polyfit(hourly_data['temp_celsius'].dropna(), \n",
    "                   hourly_data.loc[hourly_data['temp_celsius'].notna(), 'trip_count'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax2.plot(hourly_data['temp_celsius'].sort_values(), \n",
    "             p(hourly_data['temp_celsius'].sort_values()), \n",
    "             \"r--\", alpha=0.8, linewidth=2, \n",
    "             label=f'r = {correlations[\"temp_celsius\"]:.3f}')\n",
    "    ax2.set_xlabel('Temperature (°C)')\n",
    "    ax2.set_ylabel('Trip Count')\n",
    "    ax2.set_title('Temperature vs Trip Demand', fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cycling Score scatter\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.scatter(hourly_data['cycling_score'], hourly_data['trip_count'], alpha=0.5, s=20, color='green')\n",
    "    z = np.polyfit(hourly_data['cycling_score'].dropna(), \n",
    "                   hourly_data.loc[hourly_data['cycling_score'].notna(), 'trip_count'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax3.plot(hourly_data['cycling_score'].sort_values(), \n",
    "             p(hourly_data['cycling_score'].sort_values()), \n",
    "             \"r--\", alpha=0.8, linewidth=2, \n",
    "             label=f'r = {correlations[\"cycling_score\"]:.3f}')\n",
    "    ax3.set_xlabel('Cycling Score (0-100)')\n",
    "    ax3.set_ylabel('Trip Count')\n",
    "    ax3.set_title('Cycling Score vs Trip Demand', fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Heatmap\n",
    "    ax4 = axes[1, 1]\n",
    "    corr_matrix = hourly_data[['trip_count'] + weather_vars].corr()\n",
    "    trip_corr = corr_matrix[['trip_count']].drop('trip_count')\n",
    "    sns.heatmap(trip_corr, annot=True, fmt='.3f', cmap='RdYlGn', center=0, \n",
    "                ax=ax4, cbar_kws={'label': 'Correlation'}, vmin=-1, vmax=1)\n",
    "    ax4.set_title('Weather Correlation Heatmap', fontweight='bold')\n",
    "    ax4.set_ylabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    pdf.savefig(fig)\n",
    "    plt.close()\n",
    "    \n",
    "    # Set PDF metadata\n",
    "    d = pdf.infodict()\n",
    "    d['Title'] = 'Bike Sharing Analysis - Complete Results'\n",
    "    d['Author'] = 'Travel Pattern Discovery Analysis'\n",
    "    d['Subject'] = 'Comprehensive analysis of bike sharing data with weather and clustering'\n",
    "    d['Keywords'] = 'Bike Sharing, Pattern Mining, Clustering, Weather Analysis'\n",
    "    d['CreationDate'] = datetime.now()\n",
    "\n",
    "print(f\"\\n✓ PDF export complete!\")\n",
    "print(f\"✓ File saved as: {pdf_filename}\")\n",
    "print(f\"✓ Location: {os.path.abspath(pdf_filename)}\")\n",
    "print(f\"\\nThe PDF includes:\")\n",
    "print(\"  - Summary statistics and overview\")\n",
    "print(\"  - Top routes and bidirectional analysis\")\n",
    "print(\"  - Zone movement heatmaps\")\n",
    "print(\"  - Association rules visualization\")\n",
    "print(\"  - Commute pattern distribution\")\n",
    "print(\"  - Clustering analysis (dendrogram, PCA, metrics)\")\n",
    "print(\"  - Temporal demand patterns\")\n",
    "print(\"  - Weather correlation analysis\")\n",
    "print(\"  - All detailed statistics and tables\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
